**1. What is hierarchical clustering ? Explain any two techniques for finding distance between the clusters in hierarchical clustering ?**

- Hierarchical methods divides all the data sets into hierarchy or tree structure where their are levels.
- Problem with the hierarchical methods is once the cluster formed you can not undo it.
- This type of clustering is very useful for the data visulization and summarization.


Types of Hierarchical Clustering : Agglomerative or divisive

Agglomerative: 

- An agglomerative hierarchical clustering method uses a bottom-up strategy. 
- It typically starts by letting each object form its own cluster and iteratively merges clusters
  into larger and larger clusters, until all the objects are in a single cluster or certain termination conditions are 	 satisfied
- The single cluster becomes the hierarchy’s root
- For the merging step, it finds the two clusters that are closest to each other
- Two clusters are merged per iteration, an agglomerative method requires at most n iterations.


Divisive :

- Divisive hierarchical clustering method employs a top-down strategy
- It starts by placing all objects in one cluster, which is the hierarchy’s root
- It then divides the root cluster into several smaller subclusters, and recursively partitions those clusters into
smaller ones.
- The partitioning process continues until each cluster at the lowest level is coherent enough—either containing only one object, or the objects within a cluster are sufficiently similar to each other.


![Table 4.1](/Images/Figure_5.1.png)


In both Agglomerative or Divisive hierarchical clustering, a user can specify the desired number of clusters as a termination condition.

![Table 4.1](/Images/Figure_5.2.png)

A tree structure called a dendrogram is commonly used to represent the process of
hierarchical clustering. It shows how objects are grouped together (in an agglomerative
method) or partitioned (in a divisive method) step-by-step.


![Table 4.1](/Images/Figure5.3.png)

Measures for distance between the clusters 




**2. Use any hierarchical clustering algorithm to cluster the following 8 examples into three clusters**

A1 = (2,10)
A2 = (2,5)
A3 = (8,4)
A4 = (5,8)
A5 = (7,5)
A6 = (6,4)
A7 = (1,2)
A8 = (4,9) 

Initial cluster centre A1(2, 10), A4(2,5), A7(1,2)
FIRST ITERATION
	A	B	C
A1	0	3.6	8.06
A2	5	4.24	3.16
A3	8.485	5	7.28
A4	3.605	0	7.211
A5	7.0711	3.606	6.708
A6	7.211	4.123	5.385
A7	8.062	7.211	0
A8	2.236	1.414	7.616

Cluster A (A1)
Cluster B (A4, A3, A5, A6, A8)
Cluster C (A2, A7)

Recalculate cluster centre:  
Cluster A: (2, 10)
Cluster B: (6, 6)
Cluster C: (1.5, 3.5)

SECOND ITERATION
	A	B	C
A1	0	5.65	6.519
A2	5	4.12	1.58
A3	8.48	2.82	6.52
A4	3.6	2.23	5.7
A5	7.07	1.41	5.7
A6	7.2	2	4.52
A7	8.06	6.403	1.58
A8	2.23	3.61	6.04

Cluster A (A1, A8)
Cluster B (A3, A4, A5, A6)
Cluster C (A2, A7)

Recalculate cluster centre:  

Cluster A: (3, 9.5)
Cluster B: (6.5, 5.25)
Cluster C: (1.5, 3.5)


THIRD ITERATION
	A	B	C
A1	1.12	6.54	6.52
A2	4.61	4.51	1.58
A3	7.43	1.95	6.52
A4	2.5	3.13	5.7
A5	6.02	0.56	5.7
A6	6.26	1.35	4.53
A7	7.76	6.38	1.58
A8	1.12	4.51	6.04

Cluster A (A1, A8)
Cluster B (A3, A4, A5, A6)
Cluster C (A2, A7)
As we can see there is no changes in the cluster this is the final iteration

**3. Clearly explain the DBSCAN algorithm using appropriate algorithms.**

**4. Explain BIRCH algorithm with example**

**5. What is clustering ? Explain k-means clustering alogrithm. Suppose the data for clustering **
   ** { 2, 4, 10, 12, 3, 20, 11, 25 } consider k = 2, cluster the given data using above algorithm.**


**6. Use k-means to cluster the following data into 3 clusters.**

Protin and Fat (20,9), (21,9), (15,7), (22,17), (20,8), (25,12), (26,14), (20,9), (18,9), (20,9)

**7. Explain methods of clustering?**

Partitioning methods: Given a databases of n objects or data tuples, a partitioning methods constructs k partitions of data, 
where each partition represents a cluster and k · n. Given k, the number of partitions to construct, it creates an initial 
partitioning. It then uses an iterative relocation technique that attempts to improve the partitioning by moving objects from 
one group to another. The general criterion of a good partitioning is that objects in the same cluster are \close" or related 
to each other, whereas objects of di®erent clusters are \far apart". The k-means algorithm is a commonly used partitioning method.

Hierarchical methods: A hierarchical method creates a hierarchical decomposition of the given set of data objects. It can be 
either agglomerative or divisive. The agglomerative (bottom-up) approach starts with each object forming a separate group. 
It successively merges the objects that are close to one another, until all of the groups are merged into one, or until a termination 
condition holds. The divisive (top-down) approach starts with all of the objects in the same cluster. In each successive iteration, a 
cluster is split up into smaller clusters, until eventually each object forms its own cluster or until a termination condition holds. 
AGNES and DIANA are examples of hierarchical clustering. BIRCH integrates hierarchical clustering with iterative (distance-based) 
relocation.

Density-based methods: These methods are based on the notion of density. The main idea is to continue growing a given cluster as 
long as the density in its \neighborhood" exceeds some threshold. That is, for each data point within a given cluster, the neighborhood 
of a given radius has to contain at least a minimum number of points. This method can be used to ¯lter out noise and discover clusters 
of arbitrary shape. DBSCAN and OPTICS are typical examples of density-based clustering.

Grid-based methods: Such methods quantize the object space into a ¯nite number of cells that form a grid structure. All of the 
clustering operations are performed on the grid structure. The main advantage of this approach is its fast processing time, 
which is typically independent of the number of data objects and dependent only on the number of cells in each dimension in the 
quantized space. STING is an example of grid-based clustering.

Model-based methods: This approach hypothesizes a model for each of the clusters and finds the best ¯t of the data to the given model. 
A model-based algorithm may locate clusters by constructing a density function that re°ects the spatial distribution of the data points. 
It also leads to a way of automatically determining the number of clusters based on standard statistics. It takes \noise" or outliers 
into account, therein contributing to the robustness of the approach. COBWEB and self-organizing feature maps are examples of 
model-based clustering.

Methods for high-dimensional data: High-dimensional data can typically have many irrelevant dimensions. As the dimensionality increases, 
the data usually become increasingly sparse because the data points are likely located in di®erent dimensional subspaces. The distance 
measurement between pairs of points become meaningless and the average density of points anywhere in the data is likely to be low.
Distance- and density-based clustering methods are therefore ine®ective for clustering high- dimensional data. Alternative approaches 
have been proposed, such as subspace clustering methods, which search for clusters in subspaces (or subsets of dimensions) of the data, 
rather than over the entire data space. CLIQUE and PROCLUS are examples of subspace clustering methods. Frequent pattern-based 
clustering is another clustering methodology, which extracts distinct frequent patterns among subsets of dimensions that occur 
frequently. pCluster is an example of frequent pattern-based clustering that groups objects based on their pattern similarity.

Constraint-based methods: These perform clustering by incorporating user-speci¯ed or application oriented constraints. A constraint 
can express a user's expectation or describe \properties" of the desired clustering results, and provides an e®ective means for 
communicating with the clustering process. Constraint-based methods are used in spatial clustering for clustering with obstacle 
objects (e.g., considering obstacles such as rivers and highways when planning the placement of automated banking machines) and 
user-constrained cluster analysis (e.g, considering speci¯c constraints regarding customer groups when determining the best 
location for a new service station, such as ``must serve at least 100 high-value customers"). In addition, semi-supervised 
clustering employs, for example, pairwise constraints (such as pairs of instances labeled as belonging to the same or different 
clusters) in order to improve the quality of the resulting clustering.
 


