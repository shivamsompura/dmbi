**1. What is Data Preprocessing ? Explain different methods for Data cleansing phase.**
1. Data have quality if they satisfy the requirements of the intended use. There are many factors comprising data quality,        including accuracy, completeness, consistency, etc.
2. Sometimes, the data you wish to analyze by using data mining techniques are incomplete, inaccurate, noisy or inconsistent.
3. There are many possible reasons for faulty data, such as faulty ata collection instruments, human or computer errors, intentional submission of incorrect data by users, data transmission errors, variations in naming conventions and so on.
4. Such data needs to be preprocessed for finding out anomalies and inaccuracies. If data preprocessing is not performed, the         information obtained after data mining may be unreliable.
5. Data cleaning or data cleansing is the first step in data preprocessing and attempts to fill in missing values, smooth out noise while identifying outliers and correct inconsistencies in the data. Following are the different methods for data cleansing phase:
  a.Filling Missing Values:
    i)Ignore the tuple
    ii)Fill in the missing value manually
    iii)Use a global constant to fill in the missing value.
    iv)Use a central tendnecy measure for the attribute to fill in the missing value.
    v)Use the attribute mean or median for all samples belonging to the same class as the given tuple.
    vi)Use the most probable value to fill in the missing value.
  b.Smoothing Noisy Data:
    i)Binning: Binning methods smooth out a sorted data value by consulting its neighborhood, that is the values around it. The   sorted values are distributed into buckets or bins. Smoothing can be applied on the created bins using the bin means, bin medians or the bin boundaries.
    ii)Regression: Data smoothing can also be done using regression, a technique that conforms data values to a function.
    iii)Outlier Analysis: Outliers may be detected by clustering. Similar values are organized into groups or clusters. Intuitively, values that fall outside of the set of clusters may be considered outliers.
  c)Data Cleaning as a Process: It invovles various methods by which data inconsistency can be eliminated and data can be normalized:
    i)Discrepancy Detection
    ii)Data Scrubbing
    iii)Data Auditing

**. Why data preprocessing is required ? Explain different steps involved in the data preprocessing.*

**. Partition the given the data into 4 bins using eqi-dept binning method and perform smoothing according to the folowing methods.
Smoothing by bin means
Smoothing by bin median
Smoothing by bin boundries*
**Data : 11, 13, 13,  15, 15, 16, 19, 20, 20, 20, 21, 21, 22, 23, 24, 30, 40, 45, 45, 45, 71, 72, 73, 75**
Partitioning the data set into 4 bins
  Bin 1: 11, 13, 13, 15, 15, 16
  Bin 2: 19, 20, 20, 20, 21, 21
  Bin 3: 22, 23, 24, 30, 40, 45
  Bin 4: 45, 45, 71, 72, 73, 75
Smoothing by bin means:
  Bin 1: 12.17, 12.17, 12.17, 12.17, 12.17, 12.17
  Bin 2: 24.20, 24.20, 24.20, 24.20, 24.20, 24.20
  Bin 3: 30.67, 30.67, 30.67, 30.67, 30.67, 30.67
  Bin 4: 63.50, 63.50, 63.50, 63.50, 63.50, 63.50
Smoothing by bin median:
  Bin 1: 14, 14, 14, 14, 14, 14
  Bin 2: 20, 20, 20, 20, 20, 20
  Bin 3: 27, 27, 27, 27, 27, 27
  Bin 4: 36.50, 36.50, 36.50, 36.50, 36.50, 36.50 
Smoothing by bin boundaries:
  Bin 1: 11, 11, 11, 16, 16, 16
  Bin 2: 19, 19, 19, 19, 21, 21
  Bin 3: 22, 22, 22, 22, 45, 45
  Bin 4: 45, 45, 75, 75, 75, 75

**4. Explain data normalization ?**

->The goal of normalization is to make an entire set of values have a particular property.
Methods used for data normalization:
  1)Min-Max Normalization:
Transform the data from measured units to a new
interval from 𝑛𝑒𝑤_𝑚𝑖𝑛𝐹 to 𝑛𝑒𝑤_𝑚𝑎𝑥𝐹 for feature 𝐹:
𝑣′=[(𝑣 − 𝑚𝑖𝑛𝐹)/(𝑚𝑎𝑥𝐹 − 𝑚𝑖𝑛𝐹)]*(𝑛𝑒𝑤_𝑚𝑎𝑥𝐹 − 𝑛𝑒𝑤_𝑚𝑖𝑛𝐹) + 𝑛𝑒𝑤_𝑚𝑖𝑛𝐹
where 𝑣 is the current value of feature 𝐹.
Suppose that the minimum and maximum values for
the feature income are $120,000 and $98,000,
respectively. We would like to map income to the range
0.0,1.0 . By min-max normalization, a value of $73,600
for income is transformed to:
(73,600 − 12,000)/(98,000 − 12,000)*(1.0 − 0.0 )+ 0 = 0.716
2)z-score (zero-mean) Normalization Transform the data by converting the values to a common scale with an average of zero and a standard deviation of one. A value, 𝑣, of 𝐴 is normalized to 𝑣 ′ by computing: 𝑣 ′ = (𝑣 − 𝐹 )/𝜎𝐹 where 𝐹 and 𝜎𝐹 are the mean and standard deviation of feature 𝐹, respectively.
Suppose that the mean and standard deviation of the values for the feature income are $54,000 and $16,000, respectively. With z-score normalization, a value of $73,6000 for income is transformed to (73,600 − 54,000)/ 16,000 = 1.225
3) Decimal Scaling Normalization Transform the data by moving the decimal points of values of feature 𝐹. The number of decimal points moved depends on the maximum absolute value of 𝐹. A value 𝑣 of 𝐹 is normalized to 𝑣 ′ by computing : 𝑣 ′ = 𝑣 /10𝑗 , where 𝑗 is the smallest integer such that 𝑀𝑎𝑥(| 𝑣 ′|) < 1
Suppose that the recorded values of 𝐹 range from − 986 to 917. The maximum absolute value of 𝐹 is 986. To normalize by decimal scaling, we therefore divide each value by 1,000 (i.e., 𝑗 = 3) so that −986 normalizes to −0.986 and 917 normalizes to 0.917. 


